"""
Embedding model implementation using Hugging Face transformers.
"""

import asyncio
from concurrent.futures import ThreadPoolExecutor
from typing import Any, Dict, List, Optional, Union

import numpy as np
import torch
from transformers import AutoModel, AutoTokenizer

from memory.embeddings.base import EmbeddingModel


class TransformerEmbeddingModel(EmbeddingModel):
    """Embedding model that uses Hugging Face transformers."""

    def __init__(
        self,
        model_name: str = "bert-base-uncased",
        max_length: int = 512,
        pooling_strategy: str = "mean",
        device: Optional[str] = None,
        batch_size: int = 32,
        **kwargs,
    ):
        """
        Initialize the transformer embedding model.

        Args:
            model_name: Name of the Hugging Face model to use
            max_length: Maximum sequence length for the model
            pooling_strategy: Strategy for pooling token embeddings ('mean', 'cls', or 'pooler')
            device: Device to run the model on ('cpu', 'cuda', or None for auto-detection)
            batch_size: Batch size for processing multiple texts
        """
        self.model_name = model_name
        self.max_length = max_length
        self.pooling_strategy = pooling_strategy
        self.batch_size = batch_size

        # Configure device
        if device is None:
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self.device = device

        # Load model and tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name)
        self.model.to(self.device)
        self.model.eval()  # Set to evaluation mode

        # Thread pool for async operations
        self._executor = ThreadPoolExecutor(max_workers=4)

        # Cache embedding dimension
        with torch.no_grad():
            # Get a sample embedding to determine dimension
            sample_text = "Sample text for dimension detection"
            sample_inputs = self.tokenizer(
                sample_text,
                return_tensors="pt",
                truncation=True,
                max_length=self.max_length,
            ).to(self.device)
            sample_outputs = self.model(**sample_inputs)
            self._embedding_dim = self._pool_embeddings(sample_outputs).shape[1]

    def embed_text(self, text: Union[str, List[str]]) -> np.ndarray:
        """
        Convert text to vector embeddings.

        Args:
            text: Single text or list of texts to embed

        Returns:
            numpy.ndarray: The embedding vectors with shape (batch_size, embedding_dim)
        """
        # Handle single text input
        if isinstance(text, str):
            text = [text]

        all_embeddings = []

        # Process in batches to avoid memory issues
        for i in range(0, len(text), self.batch_size):
            batch_texts = text[i : i + self.batch_size]

            with torch.no_grad():
                inputs = self.tokenizer(
                    batch_texts,
                    return_tensors="pt",
                    padding=True,
                    truncation=True,
                    max_length=self.max_length,
                ).to(self.device)

                outputs = self.model(**inputs)
                batch_embeddings = self._pool_embeddings(outputs)
                all_embeddings.append(batch_embeddings.cpu().numpy())

        # Concatenate all batches
        return np.vstack(all_embeddings)

    def embed_text_async(self, text: Union[str, List[str]]) -> asyncio.Future:
        """
        Asynchronously convert text to vector embeddings.

        Args:
            text: Single text or list of texts to embed

        Returns:
            asyncio.Future: Future object that resolves to numpy.ndarray
        """
        loop = asyncio.get_event_loop()
        return loop.run_in_executor(self._executor, self.embed_text, text)

    def get_embedding_dim(self) -> int:
        """
        Get the dimension of the embeddings generated by this model.

        Returns:
            int: Dimension of the embedding vectors
        """
        return self._embedding_dim

    def _pool_embeddings(self, model_output) -> torch.Tensor:
        """
        Pool token embeddings to get sentence embeddings.

        Args:
            model_output: Output from the transformer model

        Returns:
            torch.Tensor: Pooled embeddings
        """
        if self.pooling_strategy == "mean":
            # Mean pooling of token embeddings
            token_embeddings = model_output.last_hidden_state
            # Create attention mask
            attention_mask = model_output.get(
                "attention_mask", torch.ones_like(token_embeddings[:, :, 0])
            )

            # Expand attention mask to match token embeddings shape
            if attention_mask.dim() == 2:
                attention_mask = attention_mask.unsqueeze(-1).expand(
                    token_embeddings.size()
                )

            # Apply mask and compute mean
            sum_embeddings = torch.sum(token_embeddings * attention_mask, 1)
            sum_mask = torch.sum(attention_mask, 1)
            sum_mask = torch.clamp(sum_mask, min=1e-9)
            return sum_embeddings / sum_mask

        elif self.pooling_strategy == "cls":
            # Use [CLS] token embedding
            return model_output.last_hidden_state[:, 0]

        elif self.pooling_strategy == "pooler":
            # Use the pooler output if available
            if hasattr(model_output, "pooler_output"):
                return model_output.pooler_output
            else:
                # Fall back to CLS token if pooler not available
                return model_output.last_hidden_state[:, 0]

        else:
            raise ValueError(f"Unsupported pooling strategy: {self.pooling_strategy}")
