{{- if .Values.inferenceService.enabled }}
{{- $gpuEnabled := .Values.inferenceService.gpu }}
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "deep-recall.fullname" . }}-inference-service
  labels:
    {{- include "deep-recall.labels" . | nindent 4 }}
    app.kubernetes.io/component: inference-service
spec:
  replicas: {{ .Values.inferenceService.replicaCount }}
  selector:
    matchLabels:
      {{- include "deep-recall.selectorLabels" . | nindent 6 }}
      app.kubernetes.io/component: inference-service
  template:
    metadata:
      labels:
        {{- include "deep-recall.selectorLabels" . | nindent 8 }}
        app.kubernetes.io/component: inference-service
    spec:
      {{- with .Values.global.imagePullSecrets }}
      imagePullSecrets:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      serviceAccountName: {{ include "deep-recall.serviceAccountName" . }}
      containers:
        - name: inference-service
          {{- if $gpuEnabled }}
          image: "{{ .Values.global.registry }}{{ .Values.inferenceService.image.repository }}:{{ .Values.inferenceService.image.tag }}"
          {{- else }}
          image: "{{ .Values.global.registry }}{{ .Values.inferenceService.image.repository }}-cpu:{{ .Values.inferenceService.image.tag }}"
          {{- end }}
          imagePullPolicy: {{ .Values.inferenceService.image.pullPolicy }}
          ports:
            - name: http
              containerPort: 8000
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /health
              port: http
            initialDelaySeconds: 60  # Models can take time to load
            periodSeconds: 20
            timeoutSeconds: 10
          readinessProbe:
            httpGet:
              path: /health
              port: http
            initialDelaySeconds: 30
            periodSeconds: 10
          resources:
            limits:
              cpu: {{ .Values.inferenceService.resources.limits.cpu }}
              memory: {{ .Values.inferenceService.resources.limits.memory }}
              {{- if $gpuEnabled }}
              nvidia.com/gpu: 1
              {{- end }}
            requests:
              cpu: {{ .Values.inferenceService.resources.requests.cpu }}
              memory: {{ .Values.inferenceService.resources.requests.memory }}
          env:
            {{- if not $gpuEnabled }}
            - name: USE_QUANTIZATION
              value: "true"
            {{- end }}
            {{- range $key, $value := .Values.inferenceService.envVars }}
            - name: {{ $key }}
              value: {{ tpl $value $ | quote }}
            {{- end }}
          volumeMounts:
            - name: config-volume
              mountPath: /app/config
            {{- if .Values.inferenceService.persistence.enabled }}
            - name: model-cache
              mountPath: {{ .Values.inferenceService.persistence.mountPath }}
            {{- end }}
      volumes:
        - name: config-volume
          configMap:
            name: {{ include "deep-recall.fullname" . }}-config
        {{- if .Values.inferenceService.persistence.enabled }}
        - name: model-cache
          persistentVolumeClaim:
            claimName: {{ include "deep-recall.fullname" . }}-model-cache
        {{- end }}
---
apiVersion: v1
kind: Service
metadata:
  name: {{ include "deep-recall.fullname" . }}-inference-service
  labels:
    {{- include "deep-recall.labels" . | nindent 4 }}
    app.kubernetes.io/component: inference-service
spec:
  type: {{ .Values.inferenceService.service.type }}
  ports:
    - port: {{ .Values.inferenceService.service.port }}
      targetPort: http
      protocol: TCP
      name: http
  selector:
    {{- include "deep-recall.selectorLabels" . | nindent 4 }}
    app.kubernetes.io/component: inference-service
{{- if .Values.inferenceService.persistence.enabled }}
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: {{ include "deep-recall.fullname" . }}-model-cache
  labels:
    {{- include "deep-recall.labels" . | nindent 4 }}
    app.kubernetes.io/component: inference-service
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: {{ .Values.global.storageClass }}
  resources:
    requests:
      storage: {{ .Values.inferenceService.persistence.size }}
{{- end }}
{{- if .Values.inferenceService.autoscaling.enabled }}
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: {{ include "deep-recall.fullname" . }}-inference-service
  labels:
    {{- include "deep-recall.labels" . | nindent 4 }}
    app.kubernetes.io/component: inference-service
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: {{ include "deep-recall.fullname" . }}-inference-service
  minReplicas: {{ .Values.inferenceService.autoscaling.minReplicas }}
  maxReplicas: {{ .Values.inferenceService.autoscaling.maxReplicas }}
  metrics:
    {{- if .Values.inferenceService.autoscaling.targetCPUUtilizationPercentage }}
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: {{ .Values.inferenceService.autoscaling.targetCPUUtilizationPercentage }}
    {{- end }}
    {{- if .Values.inferenceService.autoscaling.targetMemoryUtilizationPercentage }}
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: {{ .Values.inferenceService.autoscaling.targetMemoryUtilizationPercentage }}
    {{- end }}
{{- end }}
{{- end }} 