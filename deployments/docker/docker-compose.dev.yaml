version: '3.8'

services:
  # PostgreSQL database
  postgres:
    image: postgres:16
    container_name: deep-recall-postgres
    environment:
      POSTGRES_USER: deep_recall
      POSTGRES_PASSWORD: deep_recall_password
      POSTGRES_DB: deep_recall
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./../../database/init_db.sql:/docker-entrypoint-initdb.d/init_db.sql
      - ./../../database/schema.sql:/docker-entrypoint-initdb.d/schema.sql
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U deep_recall"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Vector database (Qdrant)
  qdrant:
    image: qdrant/qdrant:latest
    container_name: deep-recall-qdrant
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_data:/qdrant/storage
    environment:
      QDRANT_ALLOW_RECOVERY_MODE: "true"

  # Memory Service
  memory-service:
    build:
      context: ../..
      dockerfile: deployments/docker/Dockerfile.memory
    container_name: deep-recall-memory
    volumes:
      - ../../:/app
    ports:
      - "8000:8000"
    depends_on:
      postgres:
        condition: service_healthy
      qdrant:
        condition: service_started
    environment:
      DATABASE_URL: postgresql://deep_recall:deep_recall_password@postgres/deep_recall
      VECTOR_DB_URL: http://qdrant:6333
      PYTHONPATH: /app
      LOG_LEVEL: DEBUG
      OTLP_ENDPOINT: http://otel-collector:4317
    restart: on-failure

  # Inference Service (GPU version)
  inference-service-gpu:
    build:
      context: ../..
      dockerfile: deployments/docker/Dockerfile.inference
    container_name: deep-recall-inference
    profiles: ["gpu"]
    volumes:
      - ../../:/app
      - model_cache:/app/model_cache
    ports:
      - "8080:8000"
    environment:
      MEMORY_SERVICE_URL: http://memory-service:8000
      MODEL_CACHE_DIR: /app/model_cache
      MODEL_TYPE: deepseek_r1
      MODEL_CONFIG_PATH: /app/config/model_config.yaml
      LOG_LEVEL: DEBUG
      OTLP_ENDPOINT: http://otel-collector:4317
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: on-failure

  # Inference Service (CPU version)
  inference-service-cpu:
    build:
      context: ../..
      dockerfile: deployments/docker/Dockerfile.inference-cpu
    container_name: deep-recall-inference
    profiles: ["default", "cpu"]
    volumes:
      - ../../:/app
      - model_cache:/app/model_cache
    ports:
      - "8080:8000"
    environment:
      MEMORY_SERVICE_URL: http://memory-service:8000
      MODEL_CACHE_DIR: /app/model_cache
      MODEL_TYPE: deepseek_r1
      MODEL_CONFIG_PATH: /app/config/model_config.yaml
      LOG_LEVEL: DEBUG
      USE_QUANTIZATION: "true"  # Enable quantization for CPU mode
      OTLP_ENDPOINT: http://otel-collector:4317
    restart: on-failure

  # Orchestrator Service
  orchestrator:
    build:
      context: ../..
      dockerfile: deployments/docker/Dockerfile.orchestrator
    container_name: deep-recall-orchestrator
    volumes:
      - ../../:/app
    ports:
      - "8001:8001"
    depends_on:
      - memory-service
    environment:
      MEMORY_SERVICE_URL: http://memory-service:8000
      INFERENCE_SERVICE_URL: http://inference-service:8000
      LOG_LEVEL: DEBUG
      OTLP_ENDPOINT: http://otel-collector:4317
    restart: on-failure

  # Prometheus for metrics collection
  prometheus:
    image: prom/prometheus:latest
    container_name: deep-recall-prometheus
    ports:
      - "9090:9090"
    volumes:
      - prometheus_data:/prometheus
      - ./../monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    restart: on-failure

  # Grafana for dashboard visualization
  grafana:
    image: grafana/grafana:latest
    container_name: deep-recall-grafana
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./../monitoring/grafana/provisioning:/etc/grafana/provisioning
      - ./../monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards/json
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
    depends_on:
      - prometheus
    restart: on-failure

  # Node Exporter for host-level metrics
  node-exporter:
    image: prom/node-exporter:latest
    container_name: deep-recall-node-exporter
    ports:
      - "9100:9100"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.ignored-mount-points=^/(sys|proc|dev|host|etc)($$|/)'
    restart: on-failure

  # cAdvisor for container metrics
  cadvisor:
    image: gcr.io/cadvisor/cadvisor:latest
    container_name: deep-recall-cadvisor
    ports:
      - "8080:8080"
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
    devices:
      - /dev/kmsg
    restart: on-failure

  # OpenTelemetry Collector
  otel-collector:
    image: otel/opentelemetry-collector-contrib:latest
    container_name: deep-recall-otel-collector
    ports:
      - "4317:4317"  # OTLP gRPC
      - "4318:4318"  # OTLP HTTP
      - "8889:8889"  # Prometheus exporter
    volumes:
      - ./../monitoring/otel-collector-config.yaml:/etc/otel-collector-config.yaml
    command:
      - --config=/etc/otel-collector-config.yaml
    restart: on-failure

  # Jaeger for distributed tracing visualization
  jaeger:
    image: jaegertracing/all-in-one:latest
    container_name: deep-recall-jaeger
    ports:
      - "16686:16686"  # UI
      - "14250:14250"  # Model
      - "4318:4318"    # OTLP HTTP
    environment:
      - COLLECTOR_OTLP_ENABLED=true
    restart: on-failure

  # Zipkin for alternative tracing visualization
  zipkin:
    image: openzipkin/zipkin:latest
    container_name: deep-recall-zipkin
    ports:
      - "9411:9411"  # UI and API
    restart: on-failure

volumes:
  postgres_data:
  qdrant_data:
  model_cache:
  prometheus_data:
  grafana_data: 